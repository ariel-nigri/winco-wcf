#!/usr/bin/env php
<?php

require dirname(__DIR__).'/common/wcf.php';

define('MULTIPART_SIZE', 5242880);

$instances = new Instances;
$instances->worker_frontend = $my_worker_hostname;
$instances->inst_active = true;

if (!$instances->select(getDbConn()))
	die("ERROR: No active instances found in this worker.\n");

$audio  = false;
$data   = false;
$logs	= false;
$regdb  = false;
$full	= false;
$verbose = false;
$webdata = false;
$attach = false;

if (count($argv) == 1)
	die("usage: ".basename($argv[0])." [verbose] webdata full dat|data regdb logs attach audio\n");

for ($i = 1; $i < $argc; $i++) {
	switch ($argv[$i]) {
	case 'webadata':
		$webdata = true;
		break;

	case 'verbose':
		$verbose = true;
		break;

	case 'full':
		$full = true;
		break;

	case 'dat':
	case 'data':
		$data = true;
		break;

	case 'regdb':
		$regdb = true;
		break;

	case 'logs':
		$logs = true;
		break;

	case 'attach':
		$attach = true;
		break;

	case 'audio':
		$audio = true;
		break;
	}
}

// create the amazon client object.
$client = getAwsS3Client();
$client->registerStreamWrapper();

if (!$data && !$audio && !$logs && !$regdb && !$webdata && !$attach)
	exit (1);

while($instances->fetch()) {
	if ($verbose)
		echo $instances->inst_seq, "\n";

	if ($audio) {
		if ($verbose)
			echo "Synching audio\n";
		sync_by_attrib_dir($client,
			$aws_bucket,
			$instances->inst_seq,
			$instances->inst_seq."/dat/audio_files",
			"/home/instances/".$instances->inst_seq."/var/winco/dat/audio_files");

		purge_old_audio_files($instances->inst_seq);
	}

	if ($logs)
		backup_log_files($client,
				$aws_bucket,
				$instances->inst_seq.'/logs',
				'/home/instances/'.$instances->inst_seq.'/var/winco/logs');


	if ($regdb) {
		$archive = $instances->inst_seq.'/regdb.backup/regdb-'.timed_filename().".tgz";
		archive_dir($client,
			$aws_bucket,
			$archive,
			'/home/instances/'.$instances->inst_seq.'/var/winco/regdb');
	}

	if ($data) {
		backup_wtm_messages($instances->inst_seq, $full ? "full" : "incr");
		sync_dir($client,
				$aws_bucket,
				$instances->inst_seq.'/dat/backup_splitted',
				'/home/instances/'.$instances->inst_seq.'/var/winco/dat/backup_splitted',
				false);

		purge_old_msg_backup_splitted($instances->inst_seq);
	}

	if ($webdata) {
		//We only need to backup hotspot.db. The other files can be generated from the log files.
		if (file_exists('/home/instances/'.$instances->inst_seq.'/var/winco/dat/hotspot.db') )
			backup_db_file($client,
				$aws_bucket,
				$instances->inst_seq.'/dat',
				'/home/instances/'.$instances->inst_seq.'/var/winco/dat/hotspot.db',
				false);
		else {
			//if ($verbose)
			//	echo "File " . '/home/instances/'.$instances->inst_seq.'/var/winco/dat/hotspot.db' . "doesnï¿½t exist\n";
		}
	}
	
	if ($attach) {
		//Only if attach directory exists
		if (file_exists("/home/instances/".$instances->inst_seq."/var/winco/dat/attach")) {
			sync_dir_if_new($client,
								$aws_bucket,
								$instances->inst_seq."/dat/attach",
								"/home/instances/".$instances->inst_seq."/var/winco/dat/attach");

		//TODO: apager attachs antigos
		} 
	}
}
//--------------------------------------------------------------------------
//--------------------------------------------------------------------------
//--------------------------------------------------------------------------
//--------------------------------------------------------------------------
//--------------------------------------------------------------------------
//--------------------------------------------------------------------------
//
// sqlretry()
// Execs a sqlite command with up to 3 retries.
// Params:
//			- in $command - The complete command to be passed to exec()
//			- out $output - Receives the sqlite results, if there is no error
//	Returns:
//			- true - OK
//			- false - error

function sqlretry($command, &$output)
{
	$retryCount = 0;
	$sqlret = 1;
	while ($retryCount < 3 && $sqlret) {
		exec( $command, $output, $sqlret);
		if ($sqlret) {
			$retryCount++;
			sleep(15);
		}
	}
	return !$sqlret;
}

function exists_in_s3($client, $bucket, $object) {
	try {
		$result = $client->doesObjectExist($bucket, $object);
	} catch (Exception $e) {
		//echo 'Caught exception [', get_class($e), ']: ', $e->getMessage(), "\n";
		return false;
	}
	return $result;
}

function backup_log_files($client, $bucket, $rdir, $sdir) {
	@mkdir("$sdir/bcktmp");
	$pattern = '/.*\.LOG_(((\d\d)-(\d\d))_(\d\d)-(\d\d))\.LOG/';		//Defines rotated files that should be backuped only once
	foreach (glob("$sdir/*.LOG") as $filename) {
		$basename = basename($filename);
		if (preg_match($pattern, $basename)) {
			//Rotated file
			//If already exists, do nothing
			if (exists_in_s3($client, $bucket, $rdir."/${basename}.gz"))
				continue;
		}
		//Compress file
		$compressedfile = "$sdir/bcktmp/$basename.gz";
		exec("gzip -c $filename > $compressedfile ", $void, $ret);
		if ($ret) {
			echo "Error compressing file $filename";
			continue;
		}

		//Send it to S3
		$rfile = "${rdir}/" . "${basename}.gz";
		sync_file($client, $bucket, $rfile, $compressedfile);
		unlink("${compressedfile}");
	}
}

function backup_db_file($client, $bucket, $rdir, $db_file, $do_delete=false) {
	global $verbose;
	if ($verbose)
		echo "backup_db_file: $bucket, $rdir, $db_file, $do_delete\n";
	
	//Generate the dump file
	$year = date('Y');
	$month = date('m');
	$day = date('d');
	$OUTPUT =  dirname($db_file).'/'.basename($db_file, '.db'). "_${year}_${month}_${day}.sql";		//Name of backup with current date
	$cmd = "(".
					"echo '.output $OUTPUT'; ".
					"echo '.dump'".
			   ")";
	if (!sqlretry($cmd . " | sqlite3 $db_file 2> /dev/null", $void)) {
		echo "Error generating dump of $db_file";
		return false;
	}
	
	//Compress file
	exec("gzip $OUTPUT", $void, $ret);
	if ($ret) {
		echo "Error compressing file $OUTPUT";
		return false;
	}
	
	//Send it to S3
	$rfile = "${rdir}/" . basename("${OUTPUT}.gz");
	sync_file($client, $bucket, $rfile, "${OUTPUT}.gz");
	unlink("${OUTPUT}.gz");
	return true;
	
}

function sync_dir($client, $bucket, $rdir, $sdir, $do_delete=false) {

	global $verbose;

	if ($verbose)
		echo "sync_dir: $bucket, $rdir, $sdir, $do_delete\n";

	$prefixlen = strlen($rdir);

	$nretries = 3;
	do {
		$retry = false;
		$opts['multipart_upload_size'] = MULTIPART_SIZE;
		if ($verbose)
			$opts['debug'] = true;

		try {
			$client->uploadDirectory($sdir, $bucket, $rdir, $opts);
		} catch (Aws\S3\Exception\S3Exception $e) {
			$retry = true;
			echo "retry...\n";
		} catch (Guzzle\Service\Exception\CommandTransferException $e) {
			$retry = true;
			echo "retry...\n";
		} catch (Aws\S3\Exception\MalformedXMLException $e) {
			$retry = true;
			echo "retry...\n";
		} catch (Aws\Common\Exception\MultipartUploadException $e) {
			$retry = true;
			echo "retry...\n";
		} catch (Exception $e) {
			echo "Caught exception syncing {$sdir} => {$rdir} [", get_class($e), ']: ', $e->getMessage(), "\n";
		} catch (Throwable $e) {
			echo "Caught exception syncing {$sdir} => {$rdir} [", get_class($e), ']: ', $e->getMessage(), "\n";
		}
	} while ($retry && (--$nretries > 0));

	if ($do_delete) {
		$iterator = $client->getIterator('ListObjects', array(
    			'Bucket' => $bucket,
				'Prefix' => $rdir
			));

		$ary = array("ASCII", "ISO-8859-1", "UTF-8");

		$delete_list = array();
		$delete_count = 0;
		foreach ($iterator as $object) {
			$file = $sdir.'/'.substr($object['Key'], $prefixlen+1);
			if (!@stat($file)) {
		    	echo "Deleted: ".$file, " => ", mb_detect_encoding($object['Key'], $ary), "\n";
				$delete_list [] = array ( 'Key' => $object['Key'] );
				$delete_count++;
			}
		}
		if ($delete_count)
			$client->deleteObjects(array ('Bucket' => $bucket, 'Objects' => $delete_list));
	}
}

function sync_file_simple($client, $bucket, $rfile, $sfile) {

	global $verbose;

    $nretries = 3;
    do {
		$retry = false;
		try {
			$result = $client->putObject(array(
					'Bucket'     => $bucket,
					'Key'        => $rfile,
					'SourceFile' => $sfile,
			    )
			);
			if ($verbose)
	    		echo "sync_file: $bucket, $rfile, $sfile\n";
		} catch (Exception $e) {
            echo 'Caught exception [', get_class($e), ']: ', $e->getMessage(), "\n";
		}
 	} while ($retry && (--$nretries > 0));
};
function sync_file($client, $bucket, $rfile, $sfile) {

	global $verbose;

	$st = stat($sfile);

	try {
		$result = $client->headObject(array(
					'Bucket'	=> $bucket,
					'Key'		=> $rfile
				)
			);
	} catch (Exception $e) {
		if ($verbose)
			echo 'Caught exception [', get_class($e), ']: ', $e->getMessage(), "\n";
		$result = false;
	}

	if ($result) {
		$rmsize = $result['ContentLength'];
		$rmtime = @$result['Metadata']['originalmodificationdate'];
	} else {
		$rmsize = 0;
		$rmtime = 0;
	}

	// echo $st['size'],  ", ", $st['mtime'], ", ", $rmtime -  $st['mtime'], "\n";
	// echo $rmsize, ", ", $rmtime, "\n";

	if ($result && $rmsize == $st['size'] && $rmtime == $st['mtime'])
		return;

    $nretries = 3;
    do {
		$retry = false;
		try {
			$result = $client->putObject(array(
					'Bucket'     => $bucket,
					'Key'        => $rfile,
					'SourceFile' => $sfile,
					'Metadata'	 => array(
						'originalmodificationdate' => $st['mtime']
					)
			    )
			);
			if ($verbose)
	    		echo "sync_file: $bucket, $rfile, $sfile\n";
		} catch (Exception $e) {
            echo 'Caught exception [', get_class($e), ']: ', $e->getMessage(), "\n";
		}
 	} while ($retry && (--$nretries > 0));
};

function sync_by_attrib_dir($client, $bucket, $instance, $rdir, $sdir) {
	global $verbose;

	if (!@stat($sdir)) {
		if ($verbose)
			 echo "sync_by_attrib_dir: no $sdir\n";
		return;
	}

	$prefixlen = strlen($sdir);

	exec("find $sdir -type f -perm /o=r,g=r | grep -v '\.tmp$'", $src);
	foreach ($src as $file) {
		if (!$client) {
			echo "\t", $file, "\n";
			continue;
		}

		$key = $rdir."/".substr($file, $prefixlen+1);

		$count = 0;
		$err = 0;
		try {
			// copy file to storage
			$result = $client->putObject(array(
				'Bucket'     => $bucket,
				'Key'        => $key,
				'SourceFile' => $file
			    )
			);

			if ($verbose)
				echo "sync_by_attrib_dir: $file -> s3://$bucket/$key\n";

			// change original file permission
			@chmod($file, 0600);
			$count++;
		} catch (Exception $e) {
			++$err;
			echo "\texcep ", ++$err, " [", $count, "]\n";
			echo 'Caught exception [', get_class($e), ']: ', $e->getMessage(), "\n";
		}
	}
}

function purge_old_audio_files($id)
{
	global $verbose;

	$datadir='/home/instances/'.$id.'/var/winco/dat/audio_files';

	if (!@stat($datadir)) {
		if ($verbose)
			echo "purge_old_audio_files: no $datadir\n";
		return;
	}

	if (@stat($datadir."/.nopurge")) {
		if ($verbose)
			echo "purge_old_audio_files: keeping $datadir\n";
		return;
	}

	if ($verbose)
		echo "purge_old_audio_files: $id\n";

	exec("tmpwatch -am 720 $datadir");
}

function purge_old_message_backups($id)
{
	global $verbose;

	if ($verbose)
		echo "purge_old_message_backups:\n";

	$datadir='/home/instances/'.$id.'/var/winco/dat';

	$lastscm=`cd $datadir/backup/schema/; ls --sort=time -- *.schema.sql | head -1 | sed -e 's/\..*sql$//'`;
	$lastscm=str_replace("\n", '', $lastscm);

	$cmd = "find ".$datadir."/backup/schema -type f -printf \"%f\\n\" | grep -v '^".$lastscm.".schema\\.sql$' | sed -e 's/\.schema\.sql//'";
	exec($cmd, $schemas);

	foreach ($schemas as $schema) {
		$cmd = "rm ".$datadir."/backup/schema/".$schema.".schema.sql";
		if ($verbose)
			echo "\t$cmd\n";
		exec($cmd);
	    $cmd = "rm ".$datadir."/backup/dumps/".$schema."/*";
		if ($verbose)
			echo "\t$cmd\n";
		exec($cmd);
		$cmd = "rmdir ".$datadir."/backup/dumps/".$schema;
		if ($verbose)
			echo "\t$cmd\n";
		exec($cmd);
	}

	$do_delete=false;

	exec("cd $datadir/backup/dumps/$lastscm; ls --sort=time", $backup_files);

	foreach ($backup_files as $file) {
    	if ($do_delete) {
        	$cmd = "rm $datadir/backup/dumps/$lastscm/$file";
			if ($verbose)
				echo "\t$cmd\n";
			exec($cmd);
			continue;
		}

		if ($verbose)
        	echo "\tkeeping $file\n";

		if (strstr($file, "full"))
       	    $do_delete=true;
	}
}

function get_last_ids($id, &$lastmsg, &$lasttlk, $database)
{
	$datadir="/home/instances/".$id."/var/winco/dat";
	if (!file_exists($database)) {
		echo "file $database not found in get_last_id function";
		$lastmsg = $lasttlk = 0;
		return;
	}
	
	$sqlret = sqlretry(
				"(".
					"echo 'begin;';".
					"echo 'select max(id) from im_message;';".
					"echo 'select max(id) from im_talk;';".
					"echo 'rollback;';".
				") | sqlite3 $database 2> /dev/null",
				$output
			);
	if (!$sqlret)	{		//Error after 3 retries. The main reason for error is database is locked
		$lastmsg = 0;
		$lasttlk = 0;
		echo  "get_last_ids:  id = $id: Error Accessing database";
		return;
	}
	$lastmsg=$output[0];
	$lasttlk=(isset($output[1]) ? $output[1] : "");

	if (empty($lastmsg))
		$lastmsg = 0;

	if (empty($lasttlk))
		$lasttlk = 0;

}

function backup_wtm_messages($id, $btype="full") {
	global $verbose;
	global $aws_bucket;
	
	$client = $GLOBALS['client'];
	
	$year = date('Y');
	$month = date('m');
	$day = date('d');
	$datadir="/home/instances/".$id."/var/winco/dat";
	@mkdir($datadir."/backup_splitted/dumps", 0750, true);			//Make sure the output directories exist
	if ($btype == 'full') {							// Full backup
		array_map('unlink', glob("$datadir/backup_splitted/dumps/*.sql"));			//Delete old files before doing a full backup
		//Verifiying all splitted files to see if hash has changed. If so, back it up.
		$dblist = glob("${datadir}/imfilter*.db");
		foreach ($dblist as $dbfile) {
			$do_backup = false;
			$filename = basename($dbfile, '.db');
			$year_month = preg_replace('/imfilter_/', '', $filename);
			$file_year = preg_replace('/_.*$/', '', $year_month);
			$file_month = preg_replace('/.*_/', '', $year_month);
			if ($file_year == $year && $file_month == $month)
				$do_backup = true;		//Always do backup of current month
			else {							//If db of previous months have changed, do a new backup
				// test hash to see if file has changed
				$hash = md5_file($dbfile);
				$remote_hash_file = "$id/dat/backup_splitted/md5/${file_year}-${file_month}.md5";
				$gothash = true;
				$result = 0;
				try {
					$result = $client->getObject(array(
						'Bucket'     => $aws_bucket,
						'Key'        => $remote_hash_file
					)
				);
				} catch (Exception $e) {
					if ($verbose)
						echo "backup_wtm_messages_splitted : While getting hash value  $remote_hash_file: Caught exception [" . get_class($e). ']: ' . $e->getMessage();
					$do_backup = true;
					$gothash = false;
				}
				if ( !$gothash || !$result)					//CouldnÂ´t get hash file, so do backup
					$do_backup = true;
				else {
					$saved_hash = (string) $result['Body'];	//Hashes are different; so do backup
					if ($hash != $saved_hash) {
						$do_backup = true;
						
					}
				}
			}
			if (!$do_backup)
				continue;
			
			//generate backup
			get_last_ids($id, $lastmsg, $lasttlk, $dbfile);
			if ($file_year == $year && $file_month == $month) {
				
				$OUTPUT = "$datadir/backup_splitted/dumps/imfilter_${year}_${month}_${day}.$btype.sql";		//Name of current month backup also has the day of month
				//Save the last records for the incremental backups
				file_put_contents("$datadir/backup_splitted/dumps/lastregisters", "$lastmsg $lasttlk");
				
				
			} else {
				$OUTPUT = "$datadir/backup_splitted/dumps/${filename}.${btype}.sql";	
			}
				
			$cmd = "(".
					"echo '.output $OUTPUT'; ".
					"echo '.dump'".
			   ")";
				sqlretry($cmd . " | sqlite3 $dbfile 2> /dev/null", $void);
				
			// test integrity
			$ret = sqlretry ("echo 'pragma quick_check;' | sqlite3 $dbfile", $output);
			if (!$ret || $output[0] != 'ok') {
				echo "Error testing integrity check on $dbfile";
				continue;				//If error we logit and do backup on file with problem
			 }
			
			//compress
			exec("gzip $OUTPUT", $output, $ret);
			if ($ret) {
				echo "Error compressing file $OUTPUT";
				continue;
			}
			//Save hash value if not current month
			if ($file_year != $year || $file_month != $month) {
				$rfile = "$id/dat/backup_splitted/md5/${file_year}-${file_month}.md5";
				try {
					 $result = $client->putObject(array(	//Use this to upload data
						'Bucket'     => $aws_bucket,
						'Key'        => $rfile,
						'Body'		=>  $hash	
					 )
				);
				} catch (Exception $e) {
					$message = 'backup_wtm_messages_splitted: Caught exception [' . get_class($e). ']: ' . $e->getMessage();
					echo $message;
				}
			}
		}
		
	} else {
		// Do incremental backup
		$dbfile = "$datadir/imfilter_${year}_${month}.db";
		if (!file_exists($dbfile))			// If the file doesnÂ´t exist, no backup needed
			return;
		get_last_ids($id, $lastmsg, $lasttlk, $dbfile);
		$startrecords = @file_get_contents("$datadir/backup_splitted/dumps/lastregisters");
		if (! $startrecords) {
			echo  "Instance $id: Error reading last registers for incremental backup. Doing from the beginning";
			$startrecords = "0 0";
		}
		$pieces = explode(" ", $startrecords);
		$startmsg  = is_numeric($pieces[0]) ? $pieces[0] : 0 ;
		$starttlk = is_numeric($pieces[1]) ? $pieces[1] : 0 ;
		$OUTPUT = "$datadir/backup_splitted/dumps/imfilter.${lastmsg}-${lasttlk}.incr.sql";
		$backupfile = $OUTPUT;
		$cmd = "( echo 'begin;'; ";

		if ($lastmsg > $startmsg) {
			$cmd .= "echo '.output $OUTPUT'; ".
					"echo '.mode insert im_message'; ".
					"echo 'select * from im_message where id > $startmsg and id <= $lastmsg;'; ";
			$OUTPUT = false;
		}

		if ($lasttlk > $starttlk) {
			if ($OUTPUT)
				$cmd .= "echo '.output ${OUTPUT}'";

			$cmd .= "echo '.mode insert im_talk'; ".
					"echo 'select * from im_talk where id > $starttlk and id <= $lasttlk;'; ";
		}

		$cmd .= "echo 'rollback;' )";
		if (($lastmsg > $startmsg) || ($lasttlk > $starttlk)) { 
			$ret = sqlretry($cmd . " | sqlite3 $dbfile 2> /dev/null", $resp);
			if ($ret) {
				exec("gzip  $backupfile", $output, $ret);
				if ($ret) {
					echo "Error compressing file $backupfile";
					return;
				}
			} else {
				$retline = end($resp);
				echo "Error doing incremental backup instance $id: $retline";
				return;
			}
		}
	}
	//Save last records numbers
	file_put_contents("$datadir/backup_splitted/dumps/lastregisters", "$lastmsg $lasttlk");
}

function tar_dir($dfile, $sdir)
{
	global $verbose;
	if ($verbose)
		echo "tar_dir: $sdir -> $dfile\n";

	exec("tar czf $dfile -C $sdir . 2>&1", $output, $rc);
	if ($verbose)
		foreach($output as $line)
			echo "\t", $line, "\n";

	return $rc;
}

function timed_filename($now=0)
{
	if ($now == 0)
		$now = time();

	$week = date("w", $now);
	if ($week < 5)
		return date("Ym-",$now).$week;

	$ord = intval(date("j",$now)/7)+1;
	return date("Ym-w-",$now).$ord;
}

function archive_dir($client, $bucket, $rfile, $sdir)
{
	global $verbose;

	$sfile = tempnam ("/tmp", "archive");

	if (tar_dir($sfile, $sdir) != 0) {
		echo "ERROR: creating $sfile\n";
		return false;
	}
	
    $nretries = 3;
    do {
		$retry = false;
		try {
			$result = $client->putObject(array(
					'Bucket'     => $bucket,
					'Key'        => $rfile,
					'SourceFile' => $sfile,
					'Metadata'	 => array(
						'originalmodificationdate' => time()
					)
			    )
			);
			if ($verbose)
	    		echo "archive_dir: $bucket, $rfile, $sfile ($sdir)\n";
		} catch (Exception $e) {
            echo 'Caught exception [', get_class($e), ']: ', $e->getMessage(), "\n";
		}
 	} while ($retry && (--$nretries > 0));

	unlink($sfile);
}

function sync_dir_if_new($client, $bucket, $rdir, $sdir) {
	global $verbose;
	// First we need to read the last day backed up.
	//We only test files newer than last backup less one day
	$lastbackupfilename = "$sdir/LASTBACKUP";
	if (file_exists($lastbackupfilename))
		$lastbackup = file_get_contents($lastbackupfilename);
	else
		$lastbackup = strtotime('2019-01-01');
	
	$oldesttime = $lastbackup - (24  * 60 * 60); // We only check for files newer than oldest
	
	foreach (glob("$sdir/*/*") as $file) {
		$filetime = filemtime($file);
		if ($filetime < $oldesttime) {		//File too old
			if ($verbose)
				echo "$file not backuped\n";
			continue;
		}
		if ($verbose)
			echo "$file is beeing backuped\n";
		$basename = basename($file);
		$rdirposfix = basename(dirname($file));
		$rfile = "${rdir}/${rdirposfix}/${basename}";

		//If already exists, donÂ´t transfer
		if (exists_in_s3($client, $bucket, $rfile)) {
			if ($verbose)
				echo "$rfile already exists\n";
			continue;
		}
		//Send it to S3
		if ($verbose)
			echo "sync: $rfile -- $file\n";
		sync_file_simple($client, $bucket, $rfile, $file);
	}
	file_put_contents($lastbackupfilename, time());  //Save the time of backup
}

function purge_old_msg_backup_splitted($id)
{
	global $verbose;
	$datadir='/home/instances/'.$id.'/var/winco/dat';
	array_map('unlink', glob("$datadir/backup_splitted/dumps/*.gz"));		//Remove gz files. Keep file lastregisters	
}
